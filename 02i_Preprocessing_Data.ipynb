{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02i_Preprocessing_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSh0BRkM+iODlXgK3Gu88Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyaShreeram/Microlensing_with_NeuralNets/blob/master/02i_Preprocessing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2lCAVnz-9TK",
        "colab_type": "text"
      },
      "source": [
        "# 02. Functions for preprocessing, generation, and plotting data\n",
        "\n",
        "Preprocessing includes: Sampling, initialization, setting the filename, and loading data\n",
        "\n",
        "Author: Soumya Shreeram <br>\n",
        "Script adapted from: Millon Martin & Kevin MÃ¼ller <br>\n",
        "Date: 23rd February 2020 <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0AAcy_nCy3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "import importlib\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "import random\n",
        "import os\n",
        "from IPython.display import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, InputSpec\n",
        "from tensorflow.python.keras.layers import Conv1D, Conv2D\n",
        "from tensorflow.python.keras.layers import MaxPooling1D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Add, BatchNormalization, Concatenate\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghRkVvMo_ZvY",
        "colab_type": "text"
      },
      "source": [
        "### 1. Sampling, Initialization, loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQHyRY57-nko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setSamplingParameters(v_t, data_dir, euler_sampling):\n",
        "  \"\"\"\n",
        "  Function defines the values for sampling parameters and no. of pixels \n",
        "\n",
        "  Inputs:\n",
        "  @v_t :: transverse velocity\n",
        "  @euler_sampling :: data samples taken by the Euler telescope\n",
        "  @data_dir :: directory with the model light-curves for inputted v_t\n",
        "\n",
        "  Retutns:\n",
        "  @n_sample :: no. of samples in the training set\n",
        "  @n_sample_max :: option to reduce no. of samples in training set\n",
        "  @n_pix :: no. of pixels, i.e. no. of data-points in the light-curve\n",
        "  \"\"\"\n",
        "  if v_t == 500:\n",
        "    if euler_sampling: \n",
        "      n_sample, n_sample_max, n_pix = 1000, 1000, 955\n",
        "    else:\n",
        "      n_sample, n_sample_max, n_pix = 5000, 5000, 955\n",
        "\n",
        "  else:\n",
        "    n_sample, n_sample_max, n_pix = 20000, 3000, 1137\n",
        "  sample_params = [n_sample, n_sample_max, n_pix]\n",
        "  return sample_params\n",
        "\n",
        "def initializer(r_0, sample_params):\n",
        "  \"\"\"\n",
        "  Function defines the class names, categories, and initializes the data arrays\n",
        "  Input:\n",
        "  @r_0 :: arr with all the scale radii of the background quasar\n",
        "  @sample_params :: arr containing defined values for the sampled data\n",
        "\n",
        "  Returns:\n",
        "  @l_curves :: data to store the light curves\n",
        "  @class_cat :: 2D array containing the classes and categories to be classified\n",
        "  @out_categories :: output array with the categories of light cureves\n",
        "  @out_radii ::  output array with all the radii of the light curves\n",
        "  \"\"\"\n",
        "  # generate categories and class names \n",
        "  classes = [str(radius) for radius in r_0]\n",
        "  categories = np.arange(len(r_0))\n",
        "\n",
        "  # initialize data arrays to be classified\n",
        "  l_curves = np.zeros((sample_params[0]*len(r_0), sample_params[2], 1))\n",
        "  out_catergories = np.zeros(sample_params[0]*len(r_0))\n",
        "  out_radii = np.zeros(sample_params[0]*len(r_0))\n",
        "\n",
        "  class_cat = [classes, categories]\n",
        "  return class_cat, l_curves, out_catergories, out_radii\n",
        "\n",
        "def getFilename(data_dir, r, v_t, sample_params, euler_sampling):\n",
        "  \"\"\"\n",
        "  Fuctions defines the file name based on the inputted transverse velocity\n",
        "  @data_dir :: path to the directory containing the data\n",
        "  @r :: arr containing the scale radii\n",
        "  @v_t :: input value for the transverse velocity\n",
        "  \"\"\"\n",
        "  if v_t == 300:\n",
        "    filename = data_dir + 'v300/simLC_A-B_n%d_v300_R'%sample_params[0] + str(r)  + '_M0,3.pkl'\n",
        "  if v_t == 500 and euler_sampling == True:\n",
        "    filename = data_dir + 'v500_euler/simLC_A-B_n%d_v500_R'%sample_params[0] + str(r)  + '_M0,3.pkl'\n",
        "  if v_t == 500 and euler_sampling == False:\n",
        "    filename = data_dir + 'v500/simLC_A-B_n%d_v500_R'%sample_params[0] + str(r)  + '_M0,3.pkl'\n",
        "  return filename\n",
        "\n",
        "def loadData(filename, l_curves, sample_params, r, euler_sampling):\n",
        "  \"\"\"\n",
        "  Function loads the data from the data files \n",
        "  Input:\n",
        "  @l_curves :: empty and initialized arr to hold light-curve info\n",
        "  @sample_params :: arr containing defined values for the sampled data \n",
        "  @r :: iterating variable over all the scale radii\n",
        "\n",
        "  Returns @l_curves :: fills the data array with required no. of light curves\n",
        "  \"\"\"\n",
        "  l_curve_file = open(filename, 'rb')\n",
        "  l_curve_data = pickle.load(l_curve_file, encoding='latin1')\n",
        "  l_curve_file.close()\n",
        "\n",
        "  # storing the save data in separate arrays\n",
        "  if euler_sampling:\n",
        "    lc, mjhd, err_mag_ml  = l_curve_data[0], l_curve_data[1], l_curve_data[2]\n",
        "  else:\n",
        "    lc = l_curve_data\n",
        "\n",
        "  # counter makes sure background is eliminated\n",
        "  count = 0\n",
        "\n",
        "  # iterating over the number of maximum sample points\n",
        "  for i in range(sample_params[1]):\n",
        "    # gets rid of corrupted data points (None and Nan entries)\n",
        "    if np.any(lc[i]) == 1:\n",
        "      continue\n",
        "    \n",
        "    # fills l_curves with non-corrupted data points\n",
        "    l_curves[r*sample_params[0]+count, :, 0] = np.asarray(lc[i])\n",
        "    count += 1\n",
        "      \n",
        "    # checks if there are enough light-curves to exit the loop\n",
        "    if count == sample_params[0]:\n",
        "      break      \n",
        "  \n",
        "  if euler_sampling:\n",
        "    return l_curves, mjhd, err_mag_ml\n",
        "  else:\n",
        "    return l_curves"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMVWz-0i_TS3",
        "colab_type": "text"
      },
      "source": [
        "### 2. Plot properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z96ns4Ij-was",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setLabels(ax, xlabel, ylabel, ticks, legend):\n",
        "    \"\"\"\n",
        "    Function sets the axes labels, legent size, and ticks\n",
        "    \"\"\"\n",
        "    ax.set_xlabel(xlabel, fontsize=20)\n",
        "    ax.set_ylabel(ylabel, fontsize=20)\n",
        "    if legend:\n",
        "        ax.legend(fontsize=14)\n",
        "    ax.grid()\n",
        "    if ticks:\n",
        "        ax.tick_params(which='both', labelsize=18)\n",
        "    return\n",
        "\n",
        "def plotLoss(ax, history, loss_Accuracy, model_name, labels):\n",
        "    \"\"\"\n",
        "    Function plots the loss curve for training run of x epochs\n",
        "    \"\"\"\n",
        "    ax.plot(history[loss_Accuracy[0]], '#00a15b', lw=2, label=labels[0])\n",
        "    ax.plot(history[loss_Accuracy[1]], '#fc5400', lw=2, label=labels[1])\n",
        "    ax.set_title('Loss curve for %s\\n'%model_name)\n",
        "    setLabels(ax, 'Epoch', r'$\\mathcal{L}\\ \\equiv$ Categorical cross entropy', ticks=True, legend=True)\n",
        "    return\n",
        "\n",
        "def plotAccuracy(ax, history, loss_Accuracy, model_name, labels):\n",
        "    \"\"\"\n",
        "    Function plots the accuracy curve for training run of x epochs\n",
        "    \"\"\"\n",
        "    ax.plot(history[loss_Accuracy[2]],  '#00a15b', lw=2, label=labels[0])\n",
        "    ax.plot(history[loss_Accuracy[3]], '#fc5400', lw=2, label=labels[1])\n",
        "    ax.set_title('Accuracy curve for %s\\n'%model_name)\n",
        "    setLabels(ax, 'Epoch', 'Accuracy', ticks=True, legend=True)\n",
        "    return\n",
        "\n",
        "def plotConfusionMatrix(ax, confusion_probability_matrix, output_class_names, \\\n",
        "                        map_choice_index):\n",
        "  \"\"\"\n",
        "  Function plots the confusion matrix\n",
        "  \"\"\"\n",
        "  cmap_choice = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\n",
        "  img = ax.imshow(confusion_probability_matrix, interpolation='nearest', \\\n",
        "                  cmap=cmap_choice[map_choice_index])\n",
        "  ax.set_title('Normalised confusion matrix')    \n",
        "  plt.colorbar(img, ax=ax, label='%')\n",
        "  img.set_clim(0, 100)\n",
        "  \n",
        "  # set ticks \n",
        "  tick_marks = np.arange(len(output_class_names))\n",
        "  ax.set_xticks(tick_marks)\n",
        "  ax.set_yticks(tick_marks)\n",
        "\n",
        "  # sets tick labels\n",
        "  ax.set_xticklabels(output_class_names, fontsize=18)\n",
        "  ax.set_yticklabels(output_class_names, fontsize=18)\n",
        "  \n",
        "  # fills the matrix\n",
        "  fmt = '.1f'\n",
        "  thresh = confusion_probability_matrix.max() / 2.0\n",
        "  for i, j in itertools.product(range(confusion_probability_matrix.shape[0]), \\\n",
        "                                range(confusion_probability_matrix.shape[1])):\n",
        "      plt.text(j, i, format(confusion_probability_matrix[i, j], fmt),\n",
        "              horizontalalignment='center',\n",
        "              color='white' if confusion_probability_matrix[i, j] > thresh else 'black')    \n",
        "  setLabels(ax, 'True label', 'Predicted label', ticks=False, legend=False)\n",
        "  return\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSIBKQCA_MpH",
        "colab_type": "text"
      },
      "source": [
        "### 3. Generate training and testing data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFVv2fnK-zk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateTestTrain(l_curves, out_catergories, out_radii, r_0):\n",
        "  \"\"\"\n",
        "  Function to generate test and train data sets\n",
        "  @out_categories, out_radii :: arr with all the categories/radii of the light curves\n",
        "  @r_0 :: array with all the entries for the scale radius of the bkg object\n",
        "\n",
        "  Returns:\n",
        "  test/train data sets into which the categories and radii of light curves are classified\n",
        "  \"\"\"  \n",
        "  categories_idx = np.arange(len(out_catergories))\n",
        "  train_idx, test_idx = train_test_split(categories_idx, test_size=0.2)\n",
        "  \n",
        "  # train data sets\n",
        "  train_l_curves = l_curves[train_idx]\n",
        "  train_radii = out_radii[train_idx]\n",
        "  train_cat = out_catergories[train_idx]\n",
        "  # encodes categorical integer features using a one-hot scheme\n",
        "  onehot_train = tf.keras.utils.to_categorical(train_cat, num_classes = len(r_0))\n",
        "\n",
        "  # test data sets\n",
        "  test_l_curves = l_curves[test_idx]\n",
        "  test_radii = out_radii[test_idx]\n",
        "  test_cat = out_catergories[test_idx]\n",
        "  onehot_test = tf.keras.utils.to_categorical(test_cat, num_classes = len(r_0))\n",
        "\n",
        "  data_sets = [test_radii, train_radii, test_cat, train_cat]\n",
        "  return train_l_curves, test_l_curves, data_sets, onehot_train, onehot_test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}