{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dr02_Preprocessing_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnN5iSTXtY5ieJCLl6WKhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyaShreeram/Microlensing_with_NeuralNets/blob/master/dr02_Preprocessing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2lCAVnz-9TK",
        "colab_type": "text"
      },
      "source": [
        "# 02. Functions for preprocessing, generation, and plotting data\n",
        "\n",
        "Preprocessing includes: Sampling, initialization, setting the filename, and loading data\n",
        "\n",
        "Author: Soumya Shreeram <br>\n",
        "Script adapted from: Millon Martin & Kevin MÃ¼ller <br>\n",
        "Date: 23rd February 2020 <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghRkVvMo_ZvY",
        "colab_type": "text"
      },
      "source": [
        "### 1. Sampling, Initialization, loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQHyRY57-nko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setSamplingParameters(v_t, data_dir, euler_sampling):\n",
        "  \"\"\"\n",
        "  Function defines the values for sampling parameters and no. of pixels \n",
        "\n",
        "  Inputs:\n",
        "  @v_t :: transverse velocity\n",
        "  @euler_sampling :: data samples taken by the Euler telescope\n",
        "  @data_dir :: directory with the model light-curves for inputted v_t\n",
        "\n",
        "  Retutns:\n",
        "  @n_sample :: no. of samples in the training set\n",
        "  @n_sample_max :: option to reduce no. of samples in training set\n",
        "  @n_pix :: no. of pixels, i.e. no. of data-points in the light-curve\n",
        "  \"\"\"\n",
        "  if v_t == 500:\n",
        "    if euler_sampling: \n",
        "      n_sample, n_sample_max, n_pix = 10000, 10000, 955\n",
        "    else:\n",
        "      n_sample, n_sample_max, n_pix = 10000, 10000, 486\n",
        "\n",
        "  else:\n",
        "    n_sample, n_sample_max, n_pix = 20000, 3000, 1137\n",
        "  sample_params = [n_sample, n_sample_max, n_pix]\n",
        "  return sample_params\n",
        "\n",
        "def initializer(r_0, sample_params):\n",
        "  \"\"\"\n",
        "  Function defines the class names, categories, and initializes the data arrays\n",
        "  Input:\n",
        "  @r_0 :: arr with all the scale radii of the background quasar\n",
        "  @sample_params :: arr containing defined values for the sampled data\n",
        "\n",
        "  Returns:\n",
        "  @l_curves :: data to store the light curves\n",
        "  @class_cat :: 2D array containing the classes and categories to be classified\n",
        "  @out_categories :: output array with the categories of light cureves\n",
        "  @out_radii ::  output array with all the radii of the light curves\n",
        "  \"\"\"\n",
        "  # generate categories and class names \n",
        "  classes = [str(radius) for radius in r_0]\n",
        "  categories = np.arange(len(r_0))\n",
        "\n",
        "  # initialize data arrays to be classified\n",
        "  l_curves = np.zeros((sample_params[0]*len(r_0), sample_params[2], 1))\n",
        "  out_catergories = np.zeros(sample_params[0]*len(r_0))\n",
        "  out_radii = np.zeros(sample_params[0]*len(r_0))\n",
        "\n",
        "  class_cat = [classes, categories]\n",
        "  return class_cat, l_curves, out_catergories, out_radii\n",
        "\n",
        "def getFilename(data_dir, r, v_t, sample_params):\n",
        "  \"\"\"\n",
        "  Fuctions defines the file name based on the inputted transverse velocity\n",
        "  @data_dir :: path to the directory containing the data\n",
        "  @r :: arr containing the scale radii\n",
        "  @v_t :: input value for the transverse velocity\n",
        "  \"\"\"\n",
        "  if v_t == 300:\n",
        "    filename = data_dir + 'v300/simLC_A-B_n%d_v300_R'%sample_params[0] + str(r)  + '_M0,3.pkl'\n",
        "  else:\n",
        "    filename = data_dir + 'v500/simLC_A-B_n%d_v500_R'%sample_params[0] + str(r)  + '_M0,3.pkl'\n",
        "  return filename\n",
        "\n",
        "def loadData(filename, l_curves, sample_params, r):\n",
        "  \"\"\"\n",
        "  Function loads the data from the data files \n",
        "  Input:\n",
        "  @l_curves :: empty and initialized arr to hold light-curve info\n",
        "  @sample_params :: arr containing defined values for the sampled data \n",
        "  @r :: iterating variable over all the scale radii\n",
        "\n",
        "  Returns @l_curves :: fills the data array with required no. of light curves\n",
        "  \"\"\"\n",
        "  l_curve_file = open(filename, 'rb')\n",
        "  l_curve_data = pickle.load(l_curve_file, encoding='latin1')\n",
        "  l_curve_file.close()\n",
        "\n",
        "  # counter makes sure background is eliminated\n",
        "  count = 0\n",
        "\n",
        "  # iterating over the number of maximum sample points\n",
        "  for i in range(sample_params[1]):\n",
        "    \n",
        "    # gets rid of corrupted data points (None and Nan entries)\n",
        "    if np.any(l_curve_data[i]) is None:\n",
        "      continue\n",
        "    if np.any(np.isnan(np.asarray(l_curve_data[i]))):\n",
        "      continue\n",
        "\n",
        "    # fills l_curves with non-corrupted data points\n",
        "    if np.max(np.abs(l_curve_data[i])) > 0.5:\n",
        "      l_curves[r*sample_params[0]+count, :, 0] = np.asarray(l_curve_data[i])\n",
        "      count += 1\n",
        "    \n",
        "    # checks if there are enough light-curves to exit the loop\n",
        "    if count == sample_params[0]:\n",
        "      break      \n",
        "  return l_curves"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMVWz-0i_TS3",
        "colab_type": "text"
      },
      "source": [
        "### 2. Plot properties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z96ns4Ij-was",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setLabels(ax, xlabel, ylabel, title):\n",
        "    \"\"\"\n",
        "    Function sets the labels of the x-y axis in the plot below\n",
        "    \"\"\"\n",
        "    ax.set_ylabel(xlabel, fontsize=16)\n",
        "    ax.set_xlabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSIBKQCA_MpH",
        "colab_type": "text"
      },
      "source": [
        "### 3. Generate training and testing data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFVv2fnK-zk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateTestTrain(out_catergories, out_radii, r_0):\n",
        "  \"\"\"\n",
        "  Function to generate test and train data sets\n",
        "  @out_categories, out_radii :: arr with all the categories/radii of the light curves\n",
        "  @r_0 :: array with all the entries for the scale radius of the bkg object\n",
        "\n",
        "  Returns:\n",
        "  test/train data sets into which the categories and radii of light curves are classified\n",
        "  \"\"\"  \n",
        "  categories_idx = np.arange(len(out_catergories))\n",
        "  train_idx, test_idx = train_test_split(categories_idx, test_size=0.2)\n",
        "  \n",
        "  # train data sets\n",
        "  train_l_curves = l_curves[train_idx]\n",
        "  train_radii = out_radii[train_idx]\n",
        "  train_cat = out_catergories[train_idx]\n",
        "  # encodes categorical integer features using a one-hot scheme\n",
        "  onehot_train = tf.keras.utils.to_categorical(train_cat, num_classes = len(r_0))\n",
        "\n",
        "  # test data sets\n",
        "  test_l_curves = l_curves[test_idx]\n",
        "  test_radii = out_radii[test_idx]\n",
        "  test_cat = out_catergories[test_idx]\n",
        "  onehot_test = tf.keras.utils.to_categorical(test_cat, num_classes = len(r_0))\n",
        "\n",
        "  data_sets = [test_radii, train_radii, test_cat, train_cat]\n",
        "  return train_l_curves, test_l_curves, data_sets, onehot_train, onehot_test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}