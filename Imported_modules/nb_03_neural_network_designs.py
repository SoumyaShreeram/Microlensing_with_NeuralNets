# -*- coding: utf-8 -*-
"""03_Neural_Network_Designs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CfqYIfbDlAV62ueQgcWgFlmo0DGxZ70k

## Building the architectures for the Neural Networks

This script does the following: 
1. Defines three different Neural Networks
    
  1. **CNN1**: CNN without batch normalization
  2. **CNN2**: CNN with batch normalization
  2. **ResNet** 
2. Defines functions for compiling, displaying, and training the model
3. Handles data generation

**Author**: Soumya Shreeram <br>
**Script adapted from**: Millon Martin & Kevin MÃ¼ller <br>
**Date**: 16th March 2020

## 1. Imports
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import os
import pickle

import numpy as np
import matplotlib.pyplot as plt
import random
from IPython.display import Image

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# %tensorflow_version 1.x
import tensorflow as tf

from tensorflow.keras import Sequential
from tensorflow.keras.models import Model, model_from_json
from tensorflow.keras.layers import Input, Activation, InputSpec
from tensorflow.python.keras.layers import Conv1D, Conv2D
from tensorflow.python.keras.layers import MaxPooling1D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Flatten, Add, BatchNormalization, Concatenate
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Layer

"""### 2. Defines a modified pooling layer that outputs a 3D tensor

Note: the `keras.layers.GLobalMaxPooling2D` returns a 2D tensor of shape: `(batch_size, channels)`. However, this pooling layer outputs a shape: `(batch_size, channels, rows or cols)` based on the input of data format.
"""

class GlobalMaxPoolingSp2D(Layer):
    """Global max pooling operation for spatial data along a single dimension.
    # Arguments
        sqash_dim: A scalar
            1: Find the maximum along the columns (the output doesn't the row dimension)
            2: Find the maximum along the rows (the output doesn't the column dimension)
            Defaults: squash_dim=2
        data_format: A string,
            one of `channels_last` (default) or `channels_first`.
            The ordering of the dimensions in the inputs.
            `channels_last` corresponds to inputs with shape
            `(batch, height, width, channels)` while `channels_first`
            corresponds to inputs with shape
            `(batch, channels, height, width)`.
            It defaults to the `image_data_format` value found in your
            Keras config file at `~/.keras/keras.json`.
            If you never set it, then it will be "channels_last".
    # Input shape
        - If `data_format='channels_last'`:
            4D tensor with shape:
            `(batch_size, rows, cols, channels)`
        - If `data_format='channels_first'`:
            4D tensor with shape:
            `(batch_size, channels, rows, cols)`
    # Output shape
        - If `data_format='channels_last'`:
            3D tensor with shape:
            `(batch_size, rows or cols, channels)`
        - If `data_format='channels_last'`:
            3D tensor with shape:
            `(batch_size, channels, rows or cols)`
    """
    
  
    def __init__(self, squash_dim=2, data_format=None, **kwargs):
        if data_format is None:
          data_format = K.image_data_format()
        data_format = data_format.lower()
        if data_format not in {'channels_first', 'channels_last'}:
          raise ValueError('The `data_format` argument must be one of '
                           '"channels_first", "channels_last". Received: ' +
                           str(value))
        self.data_format = data_format
        
        self.input_spec = InputSpec(ndim=4)
        self.squash_dim = squash_dim
        super(GlobalMaxPoolingSp2D, self).__init__(**kwargs)

    def compute_output_shape(self, input_shape):
        if self.data_format == 'channels_last':
            return (input_shape[0], input_shape[3-self.squash_dim], input_shape[3])
        else:
            return (input_shape[0], input_shape[1], input_shape[4-self.squash_dim])

    def call(self, inputs):
        if self.data_format == 'channels_last':
            return K.max(inputs, axis=self.squash_dim)
        else:
            return K.max(inputs, axis=self.squash_dim+1)
      
    def get_config(self):
        config = {'data_format': self.data_format}
        base_config = super(GlobalMaxPoolingSp2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))



"""## 3. Neural Network Architectures
"""


"""### 3.2 Convolutional Neural Network: Design 2

This CNN accounts for batch normalization unlike the first case. Additionally, used Keras functional API for more flexibitity.
"""

def defineNetworkDesignCNN(num_filter, kern_size, maxpoolsize, num_hidden_nodes,\
                           dropout_ratio, shortcut_link, batch_norm, length_traj, R_value):
  """
  Function to build the model architecture, set optimizers and compile the model 
  @num_pix :: used to define the shape of the input layer  
  @num_filters :: arr with the ascending order of filters in the conv2D layers
  @kernel_size :: arr with the kernel sizes for the corresponding conv2D layers
  @maxpoolsize :: arr with the pool sizes for the corresponding conv2D layers
  @num_hidden_nodes :: no. of nodes in the FNN layer right after the CNN
  @dropout_ratio :: weight constrain on the dropout layer of the FNN 
  @shortcut :: arr that decides when to take the skip connections/shortcuts
  @bath_norm :: bool decided wether or not to normalize the output from a layer
  @r_0 :: array of all the scale radii
  
  @Returns:: model output
  """
  input_shape = (length_traj, None, 1)
  
  #Define the network architecture
  inputs = Input(shape=input_shape)
  outputs = inputs
  
  is_shortcut = False
  cur_shortcut = 0
  
  for m1 in range(len(num_filter)):
    if shortcut_link and not is_shortcut and shortcut_link[cur_shortcut] == m1:
      out_shortcut = outputs
      is_shortcut = True
      cur_shortcut += 1
      
    outputs = Conv2D(filters=num_filter[m1], kernel_size = [kern_size[m1], 1],\
                     strides= [1,1], padding='same')(outputs)
    
    if batch_norm:
      outputs = BatchNormalization()(outputs)
    
    outputs = Activation('selu')(outputs)
    
    if shortcut_link and is_shortcut and shortcut_link[cur_shortcut] == m1:
      outputs = Concatenate(3)([outputs,out_shortcut])
      is_shortcut = False
      cur_shortcut += 1
    
    if maxpoolsize[m1] is not None:
      outputs = MaxPooling2D([maxpoolsize[m1],1], strides=[maxpoolsize[m1],1])(outputs) 

  outputs = GlobalMaxPoolingSp2D()(outputs)
  outputs = Flatten()(outputs)

  outputs = Dense(num_hidden_nodes)(outputs)
  outputs = Activation('sigmoid')(outputs)
  outputs = Dropout(dropout_ratio)(outputs)

  outputs = Dense(len(R_value))(outputs)
  outputs = Activation('softmax')(outputs)

  return inputs, outputs


"""### 3.3 Residual neural network: ResNet

The function `define_network_design_resnet` is Martin and Kevin's version while in the following code block, `defineResnet` is the function that I rewrote with some minor changes.
"""

def defineNetworkDesignResnet(num_pix, num_filter, kern_size, n_block, \
                                 maxpoolsize, num_hidden, dropout_ratio, R_value,\
                                 batch_norm = True):
  "Martin-Kevin's version copy-pasted"
  input_shape = (num_pix, None, 1)
  
  #Define the network architecture
  inputs = Input(shape=input_shape)
  outputs = inputs
  
  out_shortcut = outputs
  for i in range(n_block) : 
      for m1 in range(len(num_filter)):
        outputs = Conv2D(filters=num_filter[m1], kernel_size = [kern_size[m1], 1],\
                         strides = [1,1], padding='same')(outputs)
        outputs = Activation('relu')(outputs)
      outputs = Add()([outputs,out_shortcut])
      outputs = MaxPooling2D(pool_size = [maxpoolsize,1], strides=[2,1])(outputs) 
      if batch_norm : 
            outputs = BatchNormalization()(outputs)
        
      out_shortcut = outputs

  outputs = GlobalMaxPoolingSp2D()(outputs)
  outputs = Flatten()(outputs)
  
  for i,x in enumerate(num_hidden)  : 
      outputs = Dense(x)(outputs)
      outputs = Activation('sigmoid')(outputs)
      outputs = Dropout(dropout_ratio)(outputs)

  outputs = Dense(len(R_value))(outputs)
  outputs = Activation('softmax')(outputs)

  return inputs, outputs


def compileResNet(sample_params, r_0, save_image_dir, learning_rate, momentum):
	"""
	Function builds the model Resnet defined above and compiles it used for optimization
	@sample_params :: [num_samples, num_samples_max, num_pix]
	@r_0 :: array containing all the radii to be classified
	@save_image_dir :: path to the image directory
	@learning_rate :: rate which the weights are updated during training
	@momentum :: amount that smooths the progressions of the learning algorithm

	"""
	# ResNet model parameters
	num_filter = (32, 32, 32)
	kern_size = (10,20,50)
	n_block = 5
	maxpoolsize = 3
	num_hidden_nodes =[1000.0,]
	dropout_ratio = 0.7 
	batch_norm = True
	sampling = 1
	num_pieces = 1

	# build network
	inputs, outputs = defineNetworkDesignResnet(sample_params[2], num_filter, \
  kern_size, n_block, maxpoolsize, num_hidden_nodes, dropout_ratio, r_0, batch_norm)

	# ResNet compilation parameters
	optimizer_type = SGD(lr=learning_rate, momentum = momentum)  
	loss = 'categorical_crossentropy'  
	metrics = ['categorical_accuracy'] 

	# compiles network
	model = compileDisplayNetwork(inputs, outputs, optimizer_type, loss, \
	                                 metrics,save_image_dir+'ResNet', False)

	#model.load_weights('/content/gdrive/My Drive/Colab_Notebooks/Deep_learning_for_optical_IMaging/Resnet_weights.h5')
	print('Network created')
	return model


"""## 4. Compiling and displaying the mdoel"""

def compileDisplayNetwork(inputs, outputs, optimizer_type, loss, metrics,\
                          filename, do_print_summary):
  """
  Function compiles the model and displays model summary, graph
  @input, output :: input and output layer info from the NN
  @optimizer_type, loss, metrics :: arguments used for compiling the model
  @do_print_summary :: bool that decides wether or not to display the info about the model

  @Returns :: model
  """
  model = Model(inputs=inputs, outputs=outputs)
  model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)

  if do_print_summary:
    # summarize layers
    print(model.summary())
    # plot graph
    plot_model(model, to_file=filename+'.png')
  return model

def saveModel(model, save_model_dir, model_name):
  """
  Function to save the model and it's history
  """
  # serialize model to JSON
  model_json = model.to_json()
  with open("model.json", "w") as json_file:
      json_file.write(model_json)

  # serialize weights to HDF5
  filepath = save_model_dir + "model.h5"
  model.save_weights("model.h5")
  print("Saved model to drive")
  return

"""## 5. Data generation"""

def compute_num_batch(data_out, num_inputs, batch_size, verbose):
  
  data_out_class = np.argmax(data_out, axis=1)
  num_class = data_out.shape[1]
  num_group = 0
  
  for m1 in range(num_class):
    num_group += np.floor(np.count_nonzero(data_out_class == m1)/num_inputs)
  
  if np.isinf(batch_size) or num_group < batch_size:
    batch_size = num_group
    num_batch = 1
    if verbose:
      print('A single batch per epoch. The batch size ({}) is smaller than intended!'.format(batch_size))
  else:
    num_batch = np.floor(num_group/batch_size)
    if verbose:
      print('{} batches per epoch'.format(num_batch))
    
  return int(num_batch), int(batch_size)

def data_generator(data_in, data_out, num_inputs, batch_size, num_batch):
  
  data_out_class = np.argmax(data_out, axis=1)
  num_class = data_out.shape[1]
  data_ind = np.arange(len(data_out))
  
  data_in_group = np.zeros((num_class, data_in.shape[1], num_inputs*data_in.shape[2], 1))
  
  while True:
    np.random.shuffle(data_ind)
    num_ele_per_class = np.zeros(num_class, dtype=int)
    cur_ind = 0 ;
    
    for m1 in range(num_batch):
      data_in_batch = np.zeros((batch_size, data_in.shape[1], num_inputs*data_in.shape[2], 1))
      data_out_batch = np.zeros((batch_size, data_out.shape[1]))
      
      for m2 in range(batch_size):
        while True:
          cur_class = data_out_class[data_ind[cur_ind]]
          first_ele = num_ele_per_class[cur_class]*data_in.shape[2]
          last_ele = first_ele + data_in.shape[2]
          data_in_group[cur_class, :, first_ele:last_ele, :] \
            = data_in[data_ind[cur_ind]]
          
          num_ele_per_class[cur_class] += 1
          cur_ind += 1
          
          if num_ele_per_class[cur_class] == num_inputs:
            num_ele_per_class[cur_class] = 0
            data_in_batch[m2] = data_in_group[cur_class]
            data_out_batch[m2] = data_out[data_ind[cur_ind - 1]]
            break
      
      yield data_in_batch, data_out_batch

def reshapeTrainX(trainX):
	"Function reshapes 3D input array (X, X, 1) into a 4D array (X, X, 1, 1)"
	return trainX.reshape(trainX.shape[0:2] + (1,) + (trainX.shape[2],))

def trainModel(model, trainX, trainy, batch_size = 50, epochs = 50, validation_split = 0.2, verbose = 1):
	"""
	Function trains model for fixed number of epochs
	"""
	if trainX.ndim == 3:
		trainX = reshapeTrainX(trainX)

	model_history = model.fit(trainX, trainy, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_split=validation_split, shuffle=True)
	return model, model_history

def evaluatePredictModel(model, testX, testy, verbose = 1):

	"Function evaluates and predict the performace of the model"

	testX = testX.reshape(testX.shape[0:2] + (1,) + (testX.shape[2],))
	results = model.evaluate(testX, testy, verbose = verbose)
	predictions = model.predict(testX, verbose = verbose)
	return results, predictions


"""## 6. Sample and cut trajectories"""

def prepareDataSampleCuts(data_in, sampling, num_pieces):
  
  "Prepare data with sample cuts and evaluate residuals"

  reduced_shape = np.ceil(data_in.shape[1]/sampling) - 1
  length_piece = int(round(2*reduced_shape/(num_pieces + 1)))
  data_in_prep = np.zeros((len(data_in), length_piece, num_pieces, 1))
  
  for m1 in range(len(data_in)):
    # sampling
    cur_data = data_in[m1,0:data_in.shape[1]:sampling,0]
    
    if m1 == 0:
      print('reshaped data', cur_data.shape)
    
    #The trajectory is cut such that half of the pieces is shared with each neighbor
    #For an array of length 8, if the array is cut into 3 pieces, the pieces are the following:
    #1 -> 0:3, 2 -> 2:5, 3 -> 4:7
    for m2 in range(num_pieces - 1):
      first_ele = int(round(m2*reduced_shape/(num_pieces + 1)))
      data_in_prep[m1,:,m2,0] = cur_data[first_ele : first_ele+length_piece]
    
    data_in_prep[m1,:,num_pieces-1,0] = cur_data[len(cur_data) - length_piece : len(cur_data)]
    
  return data_in_prep