# -*- coding: utf-8 -*-
"""02i_Preprocessing_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSZ_TEChsj_GgVqsUBEWrcj6jS8UlhCl

# 02. Functions for preprocessing, generation, and plotting data

Preprocessing includes: Sampling, initialization, setting the filename, and loading data

Author: Soumya Shreeram <br>
Script adapted from: Millon Martin & Kevin MÃ¼ller <br>
Date: 23rd February 2020 <br>
"""

from google.colab import drive
import os, sys
from time import sleep
import pickle
import importlib
import itertools
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 16})
import random
from IPython.display import Image

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Activation, InputSpec
from tensorflow.python.keras.layers import Conv1D, Conv2D
from tensorflow.python.keras.layers import MaxPooling1D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Flatten, Add, BatchNormalization, Concatenate
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Layer

"""### 1. Sampling, Initialization, loading data"""

def setSamplingParameters(v_t, data_dir, season_gaps):
  """
  Function defines the values for sampling parameters and no. of pixels 

  Inputs:
  @v_t :: transverse velocity
  @euler_sampling :: data samples taken by the Euler telescope
  @data_dir :: directory with the model light-curves for inputted v_t

  Retutns:
  @n_sample :: array with no. of spectra in the training set for every radius
  @n_sample_max :: integer showing the total number of spectra for all radius
  @n_pix :: no. of pixels, i.e. no. of time-points per light-curve
  """
  # loads file containing info about num of curves per radius
  if v_t == 500:
    if season_gaps:
      filename = data_dir + 'v%d_gaps/numLcurvesPerRadius_v%d_gaps.npy'%(v_t, v_t)
      total_lcurves = np.load(filename) 
      n_sample, n_sample_max, n_pix = total_lcurves, np.sum(total_lcurves), 955
    else:
      filename = data_dir + 'v%d/numLcurvesPerRadius_v%d.npy'%(v_t, v_t)
      total_lcurves = np.load(filename)
      n_sample, n_sample_max, n_pix = total_lcurves, np.sum(total_lcurves), 4546

  else:
    filename = data_dir + 'v%d/numLcurvesPerRadius_v%d.npy'%(v_t, v_t)
    total_lcurves = np.load(filename)
    n_sample, n_sample_max, n_pix = total_lcurves, np.sum(total_lcurves), 1137
  sample_params = [n_sample, n_sample_max, n_pix]
  return sample_params

def initializer(r_0, sample_params):
  """
  Function defines the class names, categories, and initializes the data arrays
  Input:
  @r_0 :: arr with all the scale radii of the background quasar
  @sample_params :: arr containing defined values for the sampled data

  Returns:
  @l_curves :: data to store the light curves
  @class_cat :: 2D array containing the classes and categories to be classified
  @out_categories :: output array with the categories of light cureves
  @out_radii ::  output array with all the radii of the light curves
  """
  # generate categories and class names 
  classes = [str(radius) for radius in r_0]
  categories = np.arange(len(r_0))

  # initialize data arrays to be classified
  l_curves = np.zeros((sample_params[1], sample_params[2], 1))
  out_catergories = np.zeros(sample_params[1])
  out_radii = np.zeros(sample_params[1])

  class_cat = [classes, categories]
  return class_cat, l_curves, out_catergories, out_radii

def getFilename(data_dir, idx, r, v_t, sample_params, season_gaps, new_data_set):
  """
  Fuctions defines the file name based on the inputted transverse velocity
  @data_dir :: path to the directory containing the data
  @r :: arr containing the scale radii
  @idx :: iterating index
  @v_t :: input value for the transverse velocity
  @new_data_set :: boolean just for comparing data sets (new data set is generated by Eric)
  """
  if v_t == 300 or v_t == 500 and season_gaps == False:
    filename = data_dir + 'v%d/simLC_A-B_n%d_v%d_R%d_M0,3.pkl'%(v_t, sample_params[0][idx], v_t, r)
    # if it is thhe new data set generated by Eric
    if new_data_set: 
      filename = data_dir + 'v%d_new/simLC_A-B_n%d_v%d_R%d_M0,3_uniform.pkl'%(v_t, sample_params[0][idx], v_t, r)
  if v_t == 500 and season_gaps == True:
    filename = data_dir + 'v%d_gaps/simLC_A-B_n%d_v%d_R%d_M0,3.pkl'%(v_t, sample_params[0][idx], v_t, r) 
    # if it is thhe new data set generated by Eric
    if new_data_set: 
      filename = data_dir + 'v%d_gaps_new/simLC_A-B_n%d_v%d_R%d_M0,3_gaps.pkl'%(v_t, sample_params[0][idx], v_t, r)
  return filename

def loadData(filename, idx, l_curves, sample_params, r, euler_sampling, new_data_set):
  """
  Function loads the data from the data files 
  Input:
  @l_curves :: empty and initialized arr to hold light-curve info
  @sample_params :: arr containing defined values for the sampled data 
  @r :: iterating variable over all the scale radii
  @new_data_set :: if using the data set generated by Eric 
  {Note: The old data set had an error in the maps so Eric's data set is used}
  Returns @l_curves :: fills the data array with required no. of light curves
  """
  l_curve_file = open(filename, 'rb')
  l_curve_data = pickle.load(l_curve_file, encoding='latin1')
  l_curve_file.close()

  # storing the save data in separate arrays
  lc, mjhd, err_mag_ml  = l_curve_data[0], l_curve_data[1], l_curve_data[2]
  if new_data_set:
    lc  = l_curve_data

  # counter keeps track of the number of curves taken
  count = 0

  # iterating over the number of maximum sample points
  for i in range(len(lc[:])):
    # gets rid of any corrupted data points (None and Nan entries)
    if np.any(lc[i]) is None:
      continue
    
    # fills l_curves with non-corrupted data points
    l_curves[r*sample_params[0][idx]+count, :, 0] = np.asarray(lc[i])
    count += 1
      
    # checks if there are enough light-curves to exit the loop
    if count == sample_params[1]:
      break 
  return l_curves, mjhd, err_mag_ml

def saveFile(data_dir, filename, array):
  "Function to save data to a pkl file"
  file_name = data_dir + filename
  with open(file_name,'wb') as f:
    pickle.dump(array, f)
  print("File saved")
  return

def loadFile(data_dir, filename):
  "Function to load data from a .pkl file"
  file_name =  data_dir+ filename
  load_file = open(file_name, 'rb')
  load_data = pickle.load(load_file)
  load_file.close()
  return load_data

def loadMjhdFile(datadir):
    """
    Function reads the sample file and outputs the mjhd, mag_ml and errors on mag_ml
    @Returns 
    @mjhd :: time
    @mag_ml :: magnitude of microlensing
    @err_mag_ml :: error on the magnitude
    """
    filename = datadir + "J0158_Euler_microlensing_upsampled_B-A.rdb"
    # open, read and extract data
    f = open(filename,"r")
    f= f.read()
    f=f.split("\n")
    data = f[2:]
    
    mjhd, mag_ml, err_mag_ml= [], [], []
    
    # fills the arrays
    for i,elem in enumerate(data):
        mjhd = np.append(mjhd,float(elem.split("\t")[0]))
        mag_ml = np.append(mag_ml, float(elem.split("\t")[1]))
        temp = elem.split("\t")[2]
        err_mag_ml= np.append(err_mag_ml,float(temp.split("\r")[0]))

    # quick check to get eliminate infinities/nans in the error array
    err_mag_ml[np.isposinf(err_mag_ml)] = 0.0001
    err_mag_ml[np.isnan(err_mag_ml)] = 0.0001
    return mjhd, err_mag_ml

def printLcurvesShape(l_curves_interpolated):
  return print("shape of light curves arrays:", np.shape(l_curves_interpolated))
  

def showProgress(idx, n):
    """
    Function prints the progress bar for a running function
    @param idx :: iterating index
    @param n :: total number of iterating variables/ total length
    """
    j = (idx+1)/n
    sys.stdout.write('\r')
    sys.stdout.write("[%-20s] %d%%" % ('='*int(20*j), 100*j))
    sys.stdout.flush()
    sleep(0.25)
    return

"""### 2. Plot properties"""

def setLabels(ax, xlabel, ylabel, ticks, legend):
    """
    Function sets the axes labels, legent size, and ticks
    """
    ax.set_xlabel(xlabel, fontsize=20)
    ax.set_ylabel(ylabel, fontsize=20)
    if legend:
        ax.legend(fontsize=14)
    ax.grid()
    if ticks:
        ax.tick_params(which='both', labelsize=18)
    return

def plotLoss(ax, history, loss_Accuracy, model_name, labels):
    """
    Function plots the loss curve for training run of x epochs
    """
    ax.plot(history[loss_Accuracy[0]], '#00a15b', lw=2, label=labels[0])
    ax.plot(history[loss_Accuracy[1]], '#fc5400', lw=2, label=labels[1])
    ax.set_title('Loss curve for %s\n'%model_name)
    setLabels(ax, 'Epoch', r'$\mathcal{L}\ \equiv$ Categorical cross entropy', ticks=True, legend=True)
    return

def plotAccuracy(ax, history, loss_Accuracy, model_name, labels):
    """
    Function plots the accuracy curve for training run of x epochs
    """
    ax.plot(history[loss_Accuracy[2]],  '#00a15b', lw=2, label=labels[0])
    ax.plot(history[loss_Accuracy[3]], '#fc5400', lw=2, label=labels[1])
    ax.set_title('Accuracy curve for %s\n'%model_name)
    setLabels(ax, 'Epoch', 'Accuracy', ticks=True, legend=True)
    return


def plotGaussianProcessesRegressor(ax, n, l_curves, y_gpr, y_std, mjhd_gpr, mjhd, l_curves_err, out_radii):
  """
  Function plots the Gaussian Processes interpolated light curves
  @n :: array dictating the light curves number to be plotted
  @l_curves :: light curves array without interpolation
  @y_gpr :: light curve interpolated with the Gaussian processes interpolation
  @y_std :: error on the interpolated light curves
  @ mjhd_gpr, mjhd :: shuffled and original time array
  """
  colors = ['#7bdbaa',  '#8b88eb']  

  # plotting light curves with GP interpolation
  ax.plot(mjhd_gpr, y_gpr, '.', color=colors[0], label='Gaussian Processes Regressor', markersize=2)

  # fill the area within 1std
  lower_bound = y_gpr-y_std
  upper_bound = y_gpr+y_std
  ax.fill_between(mjhd_gpr, lower_bound, upper_bound, color='#fa7414', alpha=0.9)

  # plotting the data points  
  ax.errorbar(mjhd, l_curves[n, :], yerr=l_curves_err, fmt='k.', markersize=4, \
              ecolor='#ad979c', label='Radius: %i'%(out_radii[n]))
  return


def plotConfusionMatrix(ax, confusion_probability_matrix, output_class_names, \
                        color_map_choice):
  """
  Function plots the confusion matrix
  @confusion_probability_matrix :: matrix containing the data of the probability matrix
  @output_class_names :: array containing the names of the classes
  @color_map_choice :: int that chooses the color of the confusion matrix
  """
  # choice of colors for the confusion matrix
  cmap_choice = [plt.cm.Blues, plt.cm.Reds, plt.cm.Greens, plt.cm.Purples]
  
  img = ax.imshow(confusion_probability_matrix, interpolation='nearest', \
                  cmap=cmap_choice[color_map_choice])
  ax.set_title('Normalised confusion matrix')    
  plt.colorbar(img, ax=ax, label='%')
  img.set_clim(0, 100)
  
  # sets ticks 
  tick_marks = np.arange(len(output_class_names))
  ax.set_xticks(tick_marks)
  ax.set_yticks(tick_marks)

  # sets tick labels
  ax.set_xticklabels(output_class_names, fontsize=18)
  ax.set_yticklabels(output_class_names, fontsize=18)
  
  # fills the matrix
  fmt = '.1f'
  thresh = confusion_probability_matrix.max() / 2.0
  for i, j in itertools.product(range(confusion_probability_matrix.shape[0]), \
                                range(confusion_probability_matrix.shape[1])):
      plt.text(j, i, format(confusion_probability_matrix[i, j], fmt),
              horizontalalignment='center',
              color='white' if confusion_probability_matrix[i, j] > thresh else 'black')    
  setLabels(ax, 'True label', 'Predicted label', ticks=False, legend=False)
  return

"""### 3. Generate training and testing data set"""

def generateTestTrain(l_curves, out_catergories, out_radii, r_0):
  """
  Function to generate test and train data sets
  @out_categories, out_radii :: arr with all the categories/radii of the light curves
  @r_0 :: array with all the entries for the scale radius of the bkg object

  Returns:
  test/train data sets into which the categories and radii of light curves are classified
  """  
  categories_idx = np.arange(len(out_catergories))
  train_idx, test_idx = train_test_split(categories_idx, test_size=0.2)
  
  # train data sets
  train_l_curves = l_curves[train_idx]
  train_radii = out_radii[train_idx]
  train_cat = out_catergories[train_idx]
  # encodes categorical integer features using a one-hot scheme
  onehot_train = tf.keras.utils.to_categorical(train_cat, num_classes = len(r_0))

  # test data sets
  test_l_curves = l_curves[test_idx]
  test_radii = out_radii[test_idx]
  test_cat = out_catergories[test_idx]
  onehot_test = tf.keras.utils.to_categorical(test_cat, num_classes = len(r_0))

  data_sets = [test_radii, train_radii, test_cat, train_cat]
  return train_l_curves, test_l_curves, onehot_train, onehot_test, train_radii, test_radii