{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Neural_Network_Designs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLIAib9E9jSCbgHTwf3owK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyaShreeram/Microlensing_with_NeuralNets/blob/master/04_Neural_Network_Designs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKLuj8uYka_K",
        "colab_type": "text"
      },
      "source": [
        "## Building the architectures for the Neural Networks\n",
        "\n",
        "This script does the following: \n",
        "1. Defines three different Neural Networks\n",
        "    \n",
        "  1. **CNN1**: CNN without batch normalization\n",
        "  2. **CNN2**: CNN with batch normalization\n",
        "  2. **ResNet** \n",
        "2. Defines functions for compiling and displaying the model\n",
        "3. Handles data generation\n",
        "\n",
        "**Author**: Soumya Shreeram <br>\n",
        "**Script adapted from**: Millon Martin & Kevin MÃ¼ller <br>\n",
        "**Date**: 16th March 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y07pySYaA4KH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MkzBx35-Uto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from IPython.display import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import (Input, Dense, Conv1D, MaxPooling1D,\n",
        "                          Dropout, Flatten, BatchNormalization)\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xtGRanzkcLj",
        "colab_type": "text"
      },
      "source": [
        "### 2. Defines a modified pooling layer that outputs a 3D tensor\n",
        "\n",
        "Note: the `keras.layers.GLobalMaxPooling2D` returns a 2D tensor of shape: `(batch_size, channels)`. However, this pooling layer outputs a shape: `(batch_size, channels, rows or cols)` based on the input of data format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDuksc6kZiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GlobalMaxPoolingSp2D(Layer):\n",
        "    \"\"\"Global max pooling operation for spatial data along a single dimension.\n",
        "    # Arguments\n",
        "        sqash_dim: A scalar\n",
        "            1: Find the maximum along the columns (the output doesn't the row dimension)\n",
        "            2: Find the maximum along the rows (the output doesn't the column dimension)\n",
        "            Defaults: squash_dim=2\n",
        "        data_format: A string,\n",
        "            one of `channels_last` (default) or `channels_first`.\n",
        "            The ordering of the dimensions in the inputs.\n",
        "            `channels_last` corresponds to inputs with shape\n",
        "            `(batch, height, width, channels)` while `channels_first`\n",
        "            corresponds to inputs with shape\n",
        "            `(batch, channels, height, width)`.\n",
        "            It defaults to the `image_data_format` value found in your\n",
        "            Keras config file at `~/.keras/keras.json`.\n",
        "            If you never set it, then it will be \"channels_last\".\n",
        "    # Input shape\n",
        "        - If `data_format='channels_last'`:\n",
        "            4D tensor with shape:\n",
        "            `(batch_size, rows, cols, channels)`\n",
        "        - If `data_format='channels_first'`:\n",
        "            4D tensor with shape:\n",
        "            `(batch_size, channels, rows, cols)`\n",
        "    # Output shape\n",
        "        - If `data_format='channels_last'`:\n",
        "            3D tensor with shape:\n",
        "            `(batch_size, rows or cols, channels)`\n",
        "        - If `data_format='channels_last'`:\n",
        "            3D tensor with shape:\n",
        "            `(batch_size, channels, rows or cols)`\n",
        "    \"\"\"\n",
        "    \n",
        "  \n",
        "    def __init__(self, squash_dim=2, data_format=None, **kwargs):\n",
        "        if data_format is None:\n",
        "          data_format = K.image_data_format()\n",
        "        data_format = data_format.lower()\n",
        "        if data_format not in {'channels_first', 'channels_last'}:\n",
        "          raise ValueError('The `data_format` argument must be one of '\n",
        "                           '\"channels_first\", \"channels_last\". Received: ' +\n",
        "                           str(value))\n",
        "        self.data_format = data_format\n",
        "        \n",
        "        self.input_spec = InputSpec(ndim=4)\n",
        "        self.squash_dim = squash_dim\n",
        "        super(GlobalMaxPoolingSp2D, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.data_format == 'channels_last':\n",
        "            return (input_shape[0], input_shape[3-self.squash_dim], input_shape[3])\n",
        "        else:\n",
        "            return (input_shape[0], input_shape[1], input_shape[4-self.squash_dim])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.data_format == 'channels_last':\n",
        "            return K.max(inputs, axis=self.squash_dim)\n",
        "        else:\n",
        "            return K.max(inputs, axis=self.squash_dim+1)\n",
        "      \n",
        "    def get_config(self):\n",
        "        config = {'data_format': self.data_format}\n",
        "        base_config = super(GlobalMaxPoolingSp2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQLKL554N4Z",
        "colab_type": "text"
      },
      "source": [
        "General functions used to add:\n",
        "*   convolutional layers\n",
        "*   max pooling layers\n",
        "*   fully connected layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOU9eMMz1bAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addConvolutionalLayers(output, num_pix, num_filters, kernel_size, API=True):\n",
        "  \"\"\"\n",
        "  Function to add convolutional and max pooling layers to the model\n",
        "  @params defined in the main CNN function buildModel---()\n",
        "  @API :: boolean that decides wether to add layers sequenctially or using API\n",
        "  \"\"\"\n",
        "  if API:\n",
        "    kernel_size = (kernel_size, 1)\n",
        "    conv = Conv2D(num_filter, kernel_size, strides= (1,1),\\\n",
        "                      padding='same')(outputs) \n",
        "    outputs = Activation('relu')(conv)\n",
        "  # adds layers sequentially\n",
        "  else: \n",
        "    in_shape = (num_pix, None, 1) \n",
        "    kernel_size = (kernel_size, 1)\n",
        "    strides = (1,1)\n",
        "    # adds convolutional layers\n",
        "    output.add(Conv2D(num_filters, kernel_size, strides=strides, \\\n",
        "                    padding='same', input_shape = in_shape, activation='relu'))\n",
        "  return output\n",
        "\n",
        "def addPoolingLayers(output, maxpoolsize, API=True):\n",
        "  \"\"\"\n",
        "  Function adds pooling layers to the CNN\n",
        "  @output :: output from the layer\n",
        "  @maxpoolsize :: pool size value\n",
        "  @API :: boolean that decides wether to add layers sequenctially or using API\n",
        "  \"\"\"\n",
        "  if maxpoolsize is not None:\n",
        "    if API:\n",
        "      poolsize = (maxpoolsize,1)\n",
        "      output = MaxPooling2D(pool_size=poolsize, strides =(maxpoolsize,1))(output)       \n",
        "  else:\n",
        "      poolsize = (maxpoolsize,1)\n",
        "      output.add(MaxPooling2D(pool_size=poolsize, strides =(maxpoolsize,1)))       \n",
        "  return output\n",
        "\n",
        "def addFullyConnectedLayers(output, num_hidden_nodes, dropout_ratio, r_0,\\\n",
        "                            API=True):\n",
        "  \"\"\"\n",
        "  Function to add fully connected layers to the model\n",
        "  @num_hidden_nodes :: no. of nodes in the layers prior to the output layer\n",
        "  @dropout_ratio :: weight constrain in the dropout layers\n",
        "  @r_0 :: len(r_0) defines the no. of nodes in the output layer\n",
        "  @API :: boolean that decides wether to add layers sequenctially or using API\n",
        "  \"\"\"\n",
        "  # post CNN; fully connected layers\n",
        "  for i, nodes in enumerate(num_hidden_nodes):\n",
        "    if API:\n",
        "      hidden = Dense(nodes, activation='sigmoid')(output)\n",
        "      activ = Activation('sigmoid')(hidden)\n",
        "      output = Dropout(dropout_ratio)(hidden)    \n",
        "    else:\n",
        "      output.add(Dense(nodes, activation='sigmoid'))\n",
        "      output.add(Dropout(dropout_ratio))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF-qfJPU_oPy",
        "colab_type": "text"
      },
      "source": [
        "## 3. Neural Network Architectures\n",
        "\n",
        "### 3.1 Convolutional Neural Network: Design 1\n",
        "\n",
        "The following uses the `keras` Sequential models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsC1T9E2-anA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildModelCNN1(num_pix, num_filters, kernel_size, maxpoolsize,\\\n",
        "                   num_hidden_nodes, r_0):\n",
        "  \"\"\"\n",
        "  Function to build the model architecture, set optimizers and compile the model \n",
        "  @num_pix :: used to define the shape of the input layer\n",
        "  @num_filters :: arr with the ascending order of filters in the conv2D layers\n",
        "  @kernel_size :: arr with the kernel sizes for the corresponding conv2D layers\n",
        "  @maxpoolsize :: arr with the pool sizes for the corresponding conv2D layers\n",
        "  @num_hidden_nodes :: no. of nodes in the FNN layer right after the CNN\n",
        "  @r_0 :: array of all the scale radii\n",
        "  \n",
        "  @Returns:: model\n",
        "  \"\"\"\n",
        "  model = Sequential()\n",
        "  \n",
        "  # adding convolutional and pooling layers\n",
        "  model = addConvolutionalLayers(model, num_pix, num_filters, kernel_size, API=False)\n",
        "  model = addPoolingLayers(model, maxpoolsize, API=False)\n",
        "  model.add(GlobalMaxPoolingSp2D())\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  # fully connected layers; added dropout these layer with weight constraint\n",
        "  model = addFullyConnectedLayers(model, num_pix, num_hidden_nodes,\\\n",
        "                                  dropout_ratio, r_0, API=False)\n",
        "  # final output layer\n",
        "  model.add(Dense(len(r_0), activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0KkJ44kC_Eh",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Convolutional Neural Network: Design 2\n",
        "\n",
        "This CNN accounts for batch normalization unlike the first case. Additionally, used Keras functional API for more flexibitity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCUBWA6OC-yS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def builfModelCNN2(num_filter, kern_size, maxpoolsize, num_hidden, \\\n",
        "                   dropout_ratio, shortcut, batch_norm, length_traj):\n",
        "  \"\"\"\n",
        "  Function to build the model architecture, set optimizers and compile the model \n",
        "  @num_filters :: arr with the ascending order of filters in the conv2D layers\n",
        "  @kernel_size :: arr with the kernel sizes for the corresponding conv2D layers\n",
        "  @maxpoolsize :: arr with the pool sizes for the corresponding conv2D layers\n",
        "  @num_hidden_nodes :: no. of nodes in the FNN layer right after the CNN\n",
        "  @dropout_ratio :: weight constrain on the dropout layer of the FNN \n",
        "  @shortcut :: arr that decides when to take the skip connections/shortcuts\n",
        "  @bath_norm :: bool decided wether or not to normalize the output from a layer\n",
        "  @r_0 :: array of all the scale radii\n",
        "  \n",
        "  @Returns:: model output\n",
        "  \"\"\"\n",
        "  #input layer\n",
        "  visible = Input(shape=(length_traj, None, 1))\n",
        "  output = visible\n",
        "\n",
        "  # skip connection variables\n",
        "  execute_skip = False\n",
        "  idx = 0\n",
        "\n",
        "  for i in range(len(num_filter)):\n",
        "    # shortcut path/ skip connection\n",
        "    if shortcut and not execute_skip and shortcut[idx] == i:\n",
        "      out_shortcut = output\n",
        "      execute_skip = True\n",
        "      idx += 1\n",
        "    \n",
        "    # feature extractors\n",
        "    conv = addConvolutionalLayers(output, num_pix, num_filters, kernel_size[i],\\\n",
        "                                  API=True)\n",
        "    # batch normalization, activation\n",
        "    if batch_norm: \n",
        "      conv = BatchNormalization()(conv)  \n",
        "    conv = Activation('selu')(conv)\n",
        "\n",
        "    # execute skip connection\n",
        "    if shortcut and execute_skip and shortcut[idx] == i:\n",
        "      conv = concatenate(3)([conv, out_shortcut])\n",
        "      execute_skip = False\n",
        "      idx += 1\n",
        "\n",
        "    # adds max pooling layers\n",
        "    conv = addPoolingLayers(conv, maxpoolsize[i], API=True)\n",
        "    \n",
        "  pool = GlobalMaxPoolingSp2D()(conv)\n",
        "  flat = Flatten()(pool)\n",
        "  \n",
        "  # fully connected layers; added dropout these layer with weight constraint\n",
        "  hidden = addFullyConnectedLayers(flat, num_hidden_nodes, dropout_ratio, r_0, \\\n",
        "                                  API=True)\n",
        "  output = Dense(len(r_0))(hidden)\n",
        "  output = Activation('hidden')(output)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMvRFfuH_ekY",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Residual neural network: ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtgQSEE5_Uy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def buildModelResNet(num_pix, num_filter, kern_size, n_block, maxpoolsize, num_hidden, \\\n",
        "                dropout_ratio, batch_norm):\n",
        "  \"\"\"\n",
        "  Function to build the model architecture, set optimizers and compile the model \n",
        "  @num_pix :: used to define the shape of the input layer\n",
        "  @num_filters :: arr with the ascending order of filters in the conv2D layers\n",
        "  @kern_size :: arr with the kernel sizes for the corresponding conv2D layers\n",
        "  @n_block :: int decides the no. of conv layers\n",
        "  @maxpoolsize :: arr with the pool sizes for the corresponding conv2D layers\n",
        "  @num_hidden :: no. of nodes in the fully connected layer right after the CNN\n",
        "  @dropout_ratio :: weight constrain on the dropout layer of the FNN \n",
        "  @bath_norm :: bool decided wether or not to normalize the output from a layer\n",
        "  @r_0 :: array of all the scale radii\n",
        "  \n",
        "  @Returns:: model output\n",
        "  \"\"\"\n",
        "  visible = Input(shape=(num_pix, None, 1))\n",
        "  shortcut = visible\n",
        "\n",
        "  for i in range(n_block):\n",
        "    # adding convolutional layers\n",
        "    conv = addConvolutionalLayers(output, num_pix, num_filters, kernel_size[i],\\\n",
        "                                    API=True)\n",
        "    conv = Add()[conv,shortcut]\n",
        "\n",
        "    # adds max pooling and normalizes the layer\n",
        "    conv = addPoolingLayers(conv, maxpoolsize[i], API=True)\n",
        "    if batch_norm: \n",
        "      conv = BatchNormalization()(conv)\n",
        "\n",
        "    # skip connection redefined\n",
        "    shortcut = conv\n",
        "\n",
        "  pool = GlobalMaxPoolingSp2D()(conv)\n",
        "  flat = Flatten()(pool)\n",
        "  \n",
        "  # fully connected layers; added dropout these layer with weight constraint\n",
        "  hidden = addFullyConnectedLayers(flat, num_hidden_nodes, dropout_ratio, r_0, \\\n",
        "                                  API=True)\n",
        "  output = Dense(len(r_0))(hidden)\n",
        "  output = Activation('softmax')(output)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1cv-4m-Wrc",
        "colab_type": "text"
      },
      "source": [
        "## 4. Compiling and displaying the mdoel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wOSXerm-foE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compileDisplayNetwork(inputs, outputs, optimizer_type, loss, metrics, \\\n",
        "                          do_print_summary):\n",
        "  \"\"\"\n",
        "  Function compiles the model and displays model summary, graph\n",
        "  @input, output :: input and output layer info from the NN\n",
        "  @optimizer_type, loss, metrics :: arguments used for compiling the model\n",
        "  @do_print_summary :: bool that decides wether or not to display the info about the model\n",
        "\n",
        "  @Returns :: model\n",
        "  \"\"\"\n",
        "  model = Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(optimizer=optimizer_type, loss=loss, metrics=metrics)\n",
        "\n",
        "  if do_print_summary:\n",
        "    # summarize layers\n",
        "    print(model.summary())\n",
        "    # plot graph\n",
        "    plot_model(model, to_file=filename+'.png')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s7qzMWgBARw",
        "colab_type": "text"
      },
      "source": [
        "## 5. Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vRAV4koBFe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_num_batch(data_out, num_inputs, batch_size, verbose):\n",
        "  \n",
        "  data_out_class = np.argmax(data_out, axis=1)\n",
        "  num_class = data_out.shape[1]\n",
        "  num_group = 0\n",
        "  \n",
        "  for m1 in range(num_class):\n",
        "    num_group += np.floor(np.count_nonzero(data_out_class == m1)/num_inputs)\n",
        "  \n",
        "  if np.isinf(batch_size) or num_group < batch_size:\n",
        "    batch_size = num_group\n",
        "    num_batch = 1\n",
        "    if verbose:\n",
        "      print('A single batch per epoch. The batch size ({}) is smaller than intended!'.format(batch_size))\n",
        "  else:\n",
        "    num_batch = np.floor(num_group/batch_size)\n",
        "    if verbose:\n",
        "      print('{} batches per epoch'.format(num_batch))\n",
        "    \n",
        "  return int(num_batch), int(batch_size)\n",
        "\n",
        "def data_generator(data_in, data_out, num_inputs, batch_size, num_batch):\n",
        "  \n",
        "  data_out_class = np.argmax(data_out, axis=1)\n",
        "  num_class = data_out.shape[1]\n",
        "  data_ind = np.arange(len(data_out))\n",
        "  \n",
        "  data_in_group = np.zeros((num_class, data_in.shape[1], num_inputs*data_in.shape[2], 1))\n",
        "  \n",
        "  while True:\n",
        "    np.random.shuffle(data_ind)\n",
        "    num_ele_per_class = np.zeros(num_class, dtype=int)\n",
        "    cur_ind = 0 ;\n",
        "    \n",
        "    for m1 in range(num_batch):\n",
        "      data_in_batch = np.zeros((batch_size, data_in.shape[1], num_inputs*data_in.shape[2], 1))\n",
        "      data_out_batch = np.zeros((batch_size, data_out.shape[1]))\n",
        "      \n",
        "      for m2 in range(batch_size):\n",
        "        while True:\n",
        "          cur_class = data_out_class[data_ind[cur_ind]]\n",
        "          first_ele = num_ele_per_class[cur_class]*data_in.shape[2]\n",
        "          last_ele = first_ele + data_in.shape[2]\n",
        "          data_in_group[cur_class, :, first_ele:last_ele, :] \\\n",
        "            = data_in[data_ind[cur_ind]]\n",
        "          \n",
        "          num_ele_per_class[cur_class] += 1\n",
        "          cur_ind += 1\n",
        "          \n",
        "          if num_ele_per_class[cur_class] == num_inputs:\n",
        "            num_ele_per_class[cur_class] = 0\n",
        "            data_in_batch[m2] = data_in_group[cur_class]\n",
        "            data_out_batch[m2] = data_out[data_ind[cur_ind - 1]]\n",
        "            break\n",
        "      \n",
        "      yield data_in_batch, data_out_batch\n",
        "\n",
        "def fit_model_generator(model, data_in, data_out, num_inputs = 1, batch_size = 50, \\\n",
        "                        epochs = 50, validation_split = 0.2, verbose = 1):\n",
        "  \n",
        "  if num_inputs == 1:\n",
        "    if data_in.ndim == 3:\n",
        "      history = model.fit(data_in.reshape(data_in.shape[0:2] + (1,) + (data_in.shape[2],)),\\\n",
        "                          data_out, batch_size=batch_size, epochs=epochs, \\\n",
        "                          verbose=verbose, validation_split=validation_split, shuffle=True)\n",
        "    else:\n",
        "      history = model.fit(data_in, data_out, batch_size=batch_size, epochs=epochs, \\\n",
        "                          verbose=verbose, validation_split=validation_split, shuffle=True)\n",
        "  else:\n",
        "    data_ind = np.arange(len(data_out))\n",
        "    data_ind_train, data_ind_valid = train_test_split(data_ind, test_size=validation_split)\n",
        "    \n",
        "    num_batch_train, batch_size_train = compute_num_batch(data_out[data_ind_train],\\\n",
        "                                                    num_inputs, batch_size, True)\n",
        "    num_batch_valid, batch_size_valid = compute_num_batch(data_out[data_ind_valid],\\\n",
        "                                                    num_inputs, np.inf, False)\n",
        "    \n",
        "    history = model.fit_generator( \\\n",
        "                generator = data_generator(data_in[data_ind_train], data_out[data_ind_train], \\\n",
        "                  num_inputs, batch_size_train, num_batch_train), \\\n",
        "                steps_per_epoch = num_batch_train, epochs = epochs, verbose =  verbose, \\\n",
        "                validation_data = data_generator(data_in[data_ind_valid], data_out[data_ind_valid], \\\n",
        "                  num_inputs, batch_size_valid, num_batch_valid), \\\n",
        "                validation_steps = num_batch_valid)\n",
        "  \n",
        "  \n",
        "  return model, history\n",
        "\n",
        "def evaluate_model(model, data_in, data_out, num_inputs = 1, verbose = 1):\n",
        "  \n",
        "  if num_inputs == 1:\n",
        "    if data_in.ndim == 3:\n",
        "      data_in = data_in.reshape(data_in.shape[0:2] + (1,) + (data_in.shape[2],))\n",
        "  else:\n",
        "    num_batch, batch_size = compute_num_batch(data_out, num_inputs, np.inf, False)\n",
        "  \n",
        "    data_gen = data_generator(data_in, data_out, num_inputs, batch_size, num_batch)\n",
        "    data_in, data_out = next(data_gen)\n",
        "  \n",
        "  return model.evaluate(data_in, data_out, verbose = verbose)\n",
        "\n",
        "def predict_model(model, data_in, data_out, num_inputs = 1, verbose = 0):\n",
        "  \n",
        "  if num_inputs == 1:\n",
        "    if data_in.ndim == 3:\n",
        "      data_in = data_in.reshape(data_in.shape[0:2] + (1,) + (data_in.shape[2],))\n",
        "  else:\n",
        "    num_batch, batch_size = compute_num_batch(data_out, num_inputs, np.inf, False)\n",
        "    \n",
        "    data_gen = data_generator(data_in, data_out, num_inputs, batch_size, num_batch)\n",
        "    data_in, data_out = next(data_gen)\n",
        "    \n",
        "  data_predict = model.predict(data_in, verbose = verbose)\n",
        "  \n",
        "  return data_predict, data_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPos-9O7BQRi",
        "colab_type": "text"
      },
      "source": [
        "## 6. Sample and cut trajectories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P41vMVyCBX2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data_sample_cut(data_in, sampling, num_pieces):\n",
        "  \n",
        "  reduced_shape = np.ceil(data_in.shape[1]/sampling) - 1\n",
        "  length_piece = int(round(2*reduced_shape/(num_pieces + 1)))\n",
        "  data_in_prep = np.zeros((len(data_in), length_piece, num_pieces, 1))\n",
        "  \n",
        "  for m1 in range(len(data_in)):\n",
        "    cur_data = data_in[m1,0:data_in.shape[1]:sampling,0]#Sampling\n",
        "    \n",
        "    if m1 == 0:\n",
        "      print(cur_data.shape)\n",
        "    \n",
        "    #The trajectory is cut such that half of the pieces is shared with each neighbor\n",
        "    #For an array of length 8, if the array is cut into 3 pieces, the pieces are the following:\n",
        "    #1 -> 0:3, 2 -> 2:5, 3 -> 4:7\n",
        "    for m2 in range(num_pieces - 1):\n",
        "      first_ele = int(round(m2*reduced_shape/(num_pieces + 1)))\n",
        "      data_in_prep[m1,:,m2,0] = cur_data[first_ele : first_ele+length_piece]\n",
        "    \n",
        "    data_in_prep[m1,:,num_pieces-1,0] = cur_data[len(cur_data) - length_piece : len(cur_data)]\n",
        "    \n",
        "  return data_in_prep"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}